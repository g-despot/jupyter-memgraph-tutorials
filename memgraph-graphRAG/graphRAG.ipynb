{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG in Memgraph\n",
    "\n",
    "In this tutorial, we will build GraphRAG using the Memgraph ecosystem and\n",
    "OpenAI. This example is based on a portion of a fixed Game of Thrones dataset,\n",
    "which will be enriched with unstructured data to create a knowledge graph. \n",
    "\n",
    "To search for relevant information, in this example we will use vector search on\n",
    "node embeddings to find schematically relevant data. Following this, the\n",
    "structured data will be extracted from the graph and passed to LLM to answer the\n",
    "question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To begin with this tutorial, you will need Docker, Python and an OpenAI API key.\n",
    "With a few small tweaks, you can adapt this setup to run on your local Ollama\n",
    "environment. \n",
    "\n",
    "First, we need to start Memgraph with the vector search capabilities. You can do\n",
    "this by running the following command: \n",
    "\n",
    "TODO: Updated the command when the vector search is available in the official\n",
    "Memgraph docker image\n",
    "```bash\n",
    "docker run -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage:exp-vector-1 --log-level=TRACE --also-log-to-stderr --telemetry-enabled=False --experimental-vector-indexes='tag__Entity__embedding__{\"dimension\":384,\"limit\":3000}'\n",
    "```\n",
    "\n",
    "You can run this command outside of this notebook. \n",
    "\n",
    "Once Memgraph is running in the background, make sure to load the initial Game\n",
    "of Thrones dataset:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "```bash\n",
    "cat ./data/memgraph-export-got.cypherl | docker run -i memgraph/mgconsole --host=localhost\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset is ingested, install a few Python packages needed to run the demo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install neo4j                   # for driver and connection to Memgraph\n",
    "%pip install sentence-transformers   # for calculating sentence embeddings\n",
    "%pip install openai                  # for access to LLM\n",
    "%pip install dotenv                  # for environment variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich knowledge graph with the embeddings \n",
    "\n",
    "Since in GraphRAG you are not writing actual Cypher queries, rather you are\n",
    "asking the questions about your domain knowledge graph in plain English. To\n",
    "retrieve relevant parts of the knowledge graph, you'll need a way to encode\n",
    "semantic meaning into the graph.  \n",
    "\n",
    "To achieve this, encode the semantic meaning into the graph so you can locate\n",
    "the semantically similar parts of the graph. \n",
    "\n",
    "There are a several approaches to consider: embedding the node labels and\n",
    "properties, embedding the triplets related to a node or embedding specific paths\n",
    "a node can take. Adding more data into embeddings requires a vector with more\n",
    "dimensions, which can be costly in terms of memory and performance. \n",
    "\n",
    "However, with this approach, you can locate semantically similar parts of the\n",
    "graph with greater accuracy. This means that for longer questions, semantic\n",
    "search is more likely to find the right part of the graph. \n",
    "\n",
    "If the semantic search misses relevant parts of the graph, the LLM will not be\n",
    "able to answer the question correctly. \n",
    "\n",
    "To illustrate a basic example, here is a function that calculates embeddings\n",
    "based on the node labels an properties: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(driver, model):\n",
    "    with driver.session() as session:\n",
    "\n",
    "        # Retrieve all nodes\n",
    "        result = session.run(\"MATCH (n) RETURN n\")\n",
    "\n",
    "        for record in result:\n",
    "            node = record[\"n\"]\n",
    "            # Combine node labels and properties into a single string\n",
    "            node_data = (\n",
    "                \" \".join(node.labels)\n",
    "                + \" \"\n",
    "                + \" \".join(f\"{k}: {v}\" for k, v in node.items())\n",
    "            )\n",
    "\n",
    "            # Compute the embedding for the node\n",
    "            node_embedding = model.encode(node_data)\n",
    "\n",
    "            # Store the embedding back into the node\n",
    "            session.run(\n",
    "                f\"MATCH (n) WHERE id(n) = {node.element_id} SET n.embedding = {node_embedding.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Set the label to Entity for all nodes\n",
    "        session.run(\"MATCH (n) SET n:Entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a node `:Character {name:\"Viserys Targaryen\"}` in the graph, the\n",
    "encoded embedding will include the label `:Charater` and the property\n",
    "`name:Viserys Targaryen`.\n",
    "\n",
    "Asking the question `Who is Viserys Targaryen?` will yield a very similar\n",
    "embedding, allowing you to locate that node in the graph. However, if you ask a\n",
    "longer question like, `To whom was Viserys Targaryen Loyal in seasone 1 of Game\n",
    "of Thrones?`, there is a chance that this question might not locate the `Viserys\n",
    "Targaryen` node in the graph due to its length and complexity. \n",
    "\n",
    "Embedding a triplet on the node will yield a better result in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the relevant part of the graph\n",
    "\n",
    "TODO: configure and set vector search index based on new release\n",
    "\n",
    "Once embeddings are calculated in your graph, you can perform a search based on\n",
    "these embeddings by using a vector search. \n",
    "\n",
    "Memgraph supports vector search starting from version 2.22.  \n",
    "\n",
    "The goal is to find the most similar node that resembles your question and to\n",
    "extract the relevant knowledge from it. The function takes the question's\n",
    "embedding and compares it to the embeddings stored on the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_node(driver, question_embedding):\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Perform the vector search on all nodes based on the question embedding\n",
    "        result = session.run(\n",
    "            f\"CALL vector_search.search('tag', 10, {question_embedding.tolist()}) YIELD * RETURN *;\"\n",
    "        )\n",
    "        nodes_data = []\n",
    "        \n",
    "        # Retrieve all similar nodes and print them\n",
    "        for record in result:\n",
    "            node = record[\"node\"]\n",
    "            properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "            node_data = {\n",
    "                \"distance\": record[\"distance\"],\n",
    "                \"id\": node.element_id,\n",
    "                \"labels\": list(node.labels),\n",
    "                \"properties\": properties,\n",
    "            }\n",
    "            nodes_data.append(node_data)\n",
    "        print(\"All similar nodes:\")\n",
    "        for node in nodes_data:\n",
    "            print(node)\n",
    "\n",
    "        # Return the most similar node\n",
    "        return nodes_data[0] if nodes_data else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the similarity between the question embeddings and node embeddings, we\n",
    "get the most similar node. This node serves as a pivot point from which we can\n",
    "pull relevant data. For example, if we are searching for information about\n",
    "`Viserys Targaryen`, we would pull data surrounding that node, making it our\n",
    "pivot node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the relevant data\n",
    "\n",
    "Once we have the pivot node, we can begin retrieving the relevant structured\n",
    "data around it. The most straightforward approach is to perform multiple hops\n",
    "from the pivot node. \n",
    "\n",
    "Here is the function that fetches the data around pivot node, a specified number\n",
    "of `hops` away from the pivot node.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_data(driver, node, hops):\n",
    "    with driver.session() as session:\n",
    "        # Retrieve the paths from the node to other nodes that are 'hops' away\n",
    "        query = (\n",
    "            f\"MATCH path=((n)-[r*..{hops}]-(m)) WHERE id(n) = {node['id']} RETURN path\"\n",
    "        )\n",
    "        result = session.run(query)\n",
    "\n",
    "        paths = []\n",
    "        for record in result:\n",
    "            path_data = []\n",
    "            for segment in record[\"path\"]:\n",
    "\n",
    "                # Process start node without 'embedding' property\n",
    "                start_node_data = {\n",
    "                    k: v for k, v in segment.start_node.items() if k != \"embedding\"\n",
    "                }\n",
    "\n",
    "                # Process relationship data\n",
    "                relationship_data = {\n",
    "                    \"type\": segment.type,\n",
    "                    \"properties\": segment.get(\"properties\", {}),\n",
    "                }\n",
    "\n",
    "                # Process end node without 'embedding' property\n",
    "                end_node_data = {\n",
    "                    k: v for k, v in segment.end_node.items() if k != \"embedding\"\n",
    "                }\n",
    "\n",
    "                # Add to path_data as a tuple (start_node, relationship, end_node)\n",
    "                path_data.append((start_node_data, relationship_data, end_node_data))\n",
    "\n",
    "            paths.append(path_data)\n",
    "\n",
    "        # Return all paths\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert a picture showing this. \n",
    "\n",
    "To avoid overloading the LLM's limited context with non-relevant data, we drop\n",
    "the embedding property from the nodes. Embeddings contain a lot of data that\n",
    "isn't particularly relevant to the LLM. \n",
    "\n",
    "## Helper functions \n",
    "\n",
    "For the LLM to understand its task, we need specific prompts. The `RAG_prompt`\n",
    "describes how the LLM should answer the question, while the `question_prompt` is\n",
    "optimized for calculating question embeddings by extracting only the key pices\n",
    "of information to improve embedding accuracy. For example, if you ask, `Who is\n",
    "Viserys Targaryen?`, only the `Viserys Targaryen` will be extracted from the\n",
    "question. Ultimately, the LLM will receive the full question back in the\n",
    "`RAG_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_prompt(question, relevance_expansion_data):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI language model. I will provide you with a question and a set of data obtained through a relevance expansion process in a graph database. The relevance expansion process finds nodes connected to a target node within a specified number of hops and includes the relationships between these nodes.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Relevance Expansion Data:\n",
    "    {relevance_expansion_data}\n",
    "\n",
    "    Based on the provided data, please answer the question, make sure to base your answers only based on the provided data. Add a context on what data did you base your answer on.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def question_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI language model. I will provide you with a question. \n",
    "    Extract the key information from the questions. The key information is important information that is required to answer the question.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    The output format should be like this: \n",
    "    Key Information: [key information 1], [key information 2], ...\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "async def get_response(client, prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting a Graph RAG answer \n",
    "\n",
    "def main():\n",
    "\n",
    "    # Create a Neo4j driver\n",
    "    driver = neo4j.GraphDatabase.driver(\"bolt://100.64.149.141:7687\", auth=(\"\", \"\"))\n",
    "\n",
    "    # Load .env file\n",
    "    load_dotenv()\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # Load the SentenceTransformer model\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "    # Compute embeddings for all nodes in the graph\n",
    "    compute_bigger_embeddings_based_on_node(\n",
    "        driver, model\n",
    "    )\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    # Ask a question  (feel free to change the question) \n",
    "    question = \"In which episode was Viserys Targaryen killed?\"\n",
    "\n",
    "    # Key information from the question \n",
    "    prompt = question_prompt(question)\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    print(response)\n",
    "    key_information = response.split(\"Key Information: \")[1].strip()\n",
    "\n",
    "    # Compute the embedding for the key information\n",
    "    question_embedding = model.encode(key_information)\n",
    "\n",
    "    # Find the most similar node to the question embedding\n",
    "    node = find_most_similar_node(driver, question_embedding)\n",
    "    if node:\n",
    "        print(\"The most similar node is:\")\n",
    "        print(node)\n",
    "\n",
    "    # Get the relevant data based on the most similar node\n",
    "    relevant_data = get_relevant_data(driver, node, hops=2)\n",
    "\n",
    "    # Show the relevant data\n",
    "    print(\"The relevant data is:\")\n",
    "    print(relevant_data)\n",
    "\n",
    "    # LLM answers the question based on the relevant data\n",
    "    prompt = RAG_prompt(question, relevant_data)\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    print(\"The response is:\")\n",
    "    print(response)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding the knowledge\n",
    "\n",
    "Let's say that now we want to expand our existing knowledge graph with\n",
    "additional information to enrich the dataset, provide more context and retrieve\n",
    "more relevant data. In this example, we will take unstructured data, such as the\n",
    "character description summary provided below, extract entities from that\n",
    "summary, generate triplets to build the knowledge graph create queries and\n",
    "eventually execute those queries in Memgraph to incorporate with the existing\n",
    "graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text summary for processing\n",
    "summary=\"Viserys Targaryen is the last living son of the former king, Aerys II Targaryen (the 'Mad King'). As one of the last known Targaryen heirs, Viserys Targaryen is obsessed with reclaiming the Iron Throne and restoring his family’s rule over Westeros. Ambitious and arrogant, he often treats his younger sister, Daenerys Targaryen, as a pawn, seeing her only as a means to gain power. His ruthless ambition leads him to make a marriage alliance with Khal Drogo, a powerful Dothraki warlord, hoping Khal Drogo will give him the army he needs. However, Viserys Targaryen’s impatience and disrespect toward the Dothraki culture lead to his downfall; he is ultimately killed by Khal Drogo in a brutal display of 'a crown for a king' – having molten gold poured over his head. Khal Drogo is a prominent warlord and leader of the Dothraki people, known for his fearsome reputation and formidable combat skills. He enters into a marriage with Princess Daenerys Targaryen as part of an alliance orchestrated by her brother, Prince Viserys Targaryen. Though initially aloof and intimidating to Daenerys Targaryen, Khal Drogo grows to care deeply for her, and their relationship evolves into one of mutual respect and love. Khal Drogo becomes devoted to Daenerys Targaryen and her dreams, including her goal of reclaiming the Iron Throne for House Targaryen. However, Khal Drogo’s fate takes a tragic turn after he sustains a serious wound in a skirmish, which becomes infected. A healer named Mirri Maz Duur performs a ritual that leaves Khal Drogo in a vegetative state, robbing him of his strength and dignity. Heartbroken, Daenerys Targaryen ends Khal Drogo’s life mercifully, signaling both the end of her first love and a significant turning point in her journey. King Joffrey Baratheon is the eldest son of Queen Cersei Lannister and, officially, King Robert Baratheon, though he is actually the result of an incestuous relationship between Queen Cersei and her twin brother, Ser Jaime Lannister. Joffrey Baratheon's personality is marked by sadism, cruelty, and impulsive behavior, traits that make him a despised ruler and widely loathed by the people of Westeros. Following King Robert Baratheon’s death, Joffrey Baratheon takes the throne and quickly reveals himself as a tyrannical ruler, prone to rash decisions and heedless of the advice given by those around him. His cruelty is particularly directed at Lady Sansa Stark, his former fiancée, whom he torments and humiliates on multiple occasions. As king, he alienates his allies and fosters unrest with his reckless brutality. Joffrey Baratheon’s reign ends abruptly when he is poisoned at his wedding feast to Margaery Tyrell, an event known as the 'Purple Wedding' which brings relief to many who suffered under his rule. His death marks a turning point in the power struggles of King’s Landing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction\n",
    "\n",
    "TODO: add links \n",
    "\n",
    "The first step in the process is to extract entities from the summary using\n",
    "SpaCy’s large language model. SpaCy is an advanced NLP (natural language\n",
    "processing) library in Python, designed for tasks such as entity recognition,\n",
    "part-of-speech tagging, and dependency parsing. It’s widely used for its speed\n",
    "and accuracy in processing text.\n",
    "\n",
    "To begin, we need to install SpaCy and the specific model we wll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install spacy_llm\n",
    "%python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from wasabi import msg\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    msg.fail(\"OPENAI_API_KEY environment variable not set. Please set it to proceed.\", exits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of extracting entities from the text is to preprocess the data before\n",
    "sending it to the GPT model, ensuring more accurate and relevant results. By\n",
    "using SpaCy, we can identify key entities such as characters, locations and\n",
    "other entities for better understanding of the context of the text.\n",
    "\n",
    "This is particularly useful because SpaCy is specifically trained to recognize\n",
    "linguistic patterns and relationships in text, which helps to isolate and\n",
    "highlight the most important pieces of information. By preprocessing the text\n",
    "this way, we ensure that the GPT model receives a more structured input, helps\n",
    "reduce noise and irrelevant data, leading to more precise and context-aware\n",
    "outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Split document into sentences\n",
    "def split_document_sent(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def process_text(text, verbose=False):\n",
    "    doc = nlp(text)\n",
    "    if verbose:\n",
    "        msg.text(f\"Text: {doc.text}\")\n",
    "        msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "    return doc\n",
    "\n",
    "# Pipeline to run entity extraction\n",
    "def extract_entities(text, verbose=False):\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "\n",
    "    sentences = split_document_sent(text)\n",
    "    for sent in sentences:\n",
    "        doc = process_text(sent, verbose)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # Store processed data for each sentence\n",
    "        processed_data.append({'text': doc.text, 'entities': entities})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open('processed_data.json', 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n",
    "    msg.text(f\"Entity counts: {entity_counts}\")\n",
    "\n",
    "# Run the pipeline on the summary text\n",
    "verbose = True\n",
    "extract_entities(summary, verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract node and relationship parameters\n",
    "\n",
    "Now that we have extracted entities from the text, we have a better\n",
    "understanding of the data and a more structured context to send to GPT model\n",
    "we'll be using. The next step is to provide the extracted JSON file to the GPT\n",
    "prompt, along with clear instructions on how to extract nodes and relationships\n",
    "from those entities. These instructions will guide the model in identifying key\n",
    "connections between the entities, which can then be used to build a knowledge\n",
    "graph. In this example, we will be using a gpt-4 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "# Load processed data from JSON\n",
    "json_path = Path(\"processed_data.json\")\n",
    "with open(json_path, \"r\") as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# Prepare nodes and relationships\n",
    "nodes = []\n",
    "relationships = []\n",
    "\n",
    "# Formulate a prompt for GPT-4\n",
    "prompt = (\n",
    "    \"Extract entities and relationships from the following JSON data. For each entry in data['entities'], \"\n",
    "    \"create a 'node' dictionary with fields 'id' (unique identifier), 'name' (entity text), and 'type' (entity label). \"\n",
    "    \"For entities that have meaningful connections, define 'relationships' as dictionaries with 'source' (source node id), \"\n",
    "    \"'target' (target node id), and 'relationship' (type of connection). Create max 30 nodes, format relationships in the format of capital letters and _ inbetween words and format the entire response in the JSON output containing only variables nodes and relationships without any text inbetween. Use following labels for nodes: Character, Title, Location, House, Death, Event, Allegiance and following relationship types: HAPPENED_IN, SIBLING_OF, PARENT_OF, MARRIED_TO, HEALED_BY, RULES, KILLED, LOYAL_TO, BETRAYED_BY. Make sure the entire JSON file fits in the output\" \n",
    "    \"JSON data:\\n\"\n",
    "    f\"{json.dumps(processed_data)}\"\n",
    ")\n",
    "\n",
    "# Call GPT-4 to analyze the JSON and extract structured nodes and relationships\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant that structures data into nodes and relationships.\"},\n",
    "              {\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "# Parse GPT-4 response and add to nodes and relationships lists\n",
    "output = response['choices'][0]['message']['content']\n",
    "print(output)\n",
    "structured_data = json.loads(output)  # Assuming GPT-4 outputs structured JSON\n",
    "\n",
    "# Populate nodes and relationships lists\n",
    "nodes.extend(structured_data.get(\"nodes\", []))\n",
    "relationships.extend(structured_data.get(\"relationships\", []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate queries\n",
    "\n",
    "Now that GPT has provided us with the structured data for the nodes and\n",
    "relationships, the next step is to generate the Cypher queries that we will use\n",
    "to execute in Memgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher_queries(nodes, relationships):\n",
    "    queries = []\n",
    "\n",
    "    # Create nodes\n",
    "    for node in nodes:\n",
    "        query = f\"\"\"\n",
    "        MERGE (n:{node['type']}:Entity {{name: '{node['name']}'}}) \n",
    "        ON CREATE SET n.id={node['id']} \n",
    "        ON MATCH SET n.id={node['id']}\n",
    "        \"\"\"\n",
    "        queries.append(query)\n",
    "\n",
    "    # Create relationships\n",
    "    for rel in relationships:\n",
    "        query = f\"MATCH (a {{id: {rel['source']}}}), (b {{id: {rel['target']}}}) \" \\\n",
    "                f\"CREATE (a)-[:{rel['relationship']}]->(b)\"\n",
    "        queries.append(query)\n",
    "\n",
    "    return queries\n",
    "\n",
    "cypher_queries = generate_cypher_queries(nodes, relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute queries\n",
    "\n",
    "The final step is to execute those queries in Memgraph, enriching your graph\n",
    "with the newly created context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize the Neo4j driver for Memgraph (modify the URI if necessary)\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# Function to execute Cypher queries in Memgraph\n",
    "def execute_cypher_queries(queries):\n",
    "    with driver.session() as session:\n",
    "        for query in queries:\n",
    "            try:\n",
    "                session.run(query)\n",
    "                msg.good(f\"Executed query: {query}\")\n",
    "            except Exception as e:\n",
    "                msg.fail(f\"Error executing query: {query}. Error: {e}\")\n",
    "\n",
    "# Execute the generated Cypher queries\n",
    "execute_cypher_queries(cypher_queries)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('kg_examp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bdc9527c232110913f250aa506ce0def2a2883fef9960f0ee91861d9de544f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
