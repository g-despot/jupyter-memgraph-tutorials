{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG in Memgraph\n",
    "\n",
    "In this tutorial, we will build GraphRAG using the Memgraph ecosystem and\n",
    "OpenAI. This example is based on a portion of a fixed Game of Thrones dataset,\n",
    "which will be enriched with unstructured data to create a knowledge graph. \n",
    "\n",
    "To search for relevant information, in this example we will use vector search on\n",
    "node embeddings to find schematically relevant data. Following this, the\n",
    "structured data will be extracted from the graph and passed to LLM to answer the\n",
    "question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To begin with this tutorial, you will need Docker, Python and an OpenAI API key.\n",
    "With a few small tweaks, you can adapt this setup to run on your local Ollama\n",
    "environment. \n",
    "\n",
    "First, we need to start Memgraph with the vector search capabilities. You can do\n",
    "this by running the following command: \n",
    "\n",
    "TODO: Updated the command when the vector search is available in the official\n",
    "Memgraph docker image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docker run -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage:exp-vector-1 --log-level=TRACE --also-log-to-stderr --telemetry-enabled=False --experimental-vector-indexes='tag__Entity__embedding__{\"dimension\":384,\"limit\":3000}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You can run this command outside of this notebook. \n",
    "\n",
    "Once Memgraph is running in the background, make sure to load the initial Game\n",
    "of Thrones dataset:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cat ./data/memgraph-export-got.cypherl | docker run -i memgraph/mgconsole --host=localhost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset is ingested, install a few Python packages needed to run the demo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install neo4j                   # for driver and connection to Memgraph\n",
    "%pip install sentence-transformers   # for calculating sentence embeddings\n",
    "%pip install openai                  # for access to LLM\n",
    "%pip install dotenv                  # for environment variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich knowledge graph with the embeddings \n",
    "\n",
    "Since in GraphRAG you are not writing actual Cypher queries, rather you are\n",
    "asking the questions about your domain knowledge graph in plain English. To\n",
    "retrieve relevant parts of the knowledge graph, you'll need a way to encode\n",
    "semantic meaning into the graph.  \n",
    "\n",
    "To achieve this, encode the semantic meaning into the graph so you can locate\n",
    "the semantically similar parts of the graph. \n",
    "\n",
    "There are a several approaches to consider: embedding the node labels and\n",
    "properties, embedding the triplets related to a node or embedding specific paths\n",
    "a node can take. Adding more data into embeddings requires a vector with more\n",
    "dimensions, which can be costly in terms of memory and performance. \n",
    "\n",
    "However, with this approach, you can locate semantically similar parts of the\n",
    "graph with greater accuracy. This means that for longer questions, semantic\n",
    "search is more likely to find the right part of the graph. \n",
    "\n",
    "If the semantic search misses relevant parts of the graph, the LLM will not be\n",
    "able to answer the question correctly. \n",
    "\n",
    "To illustrate a basic example, here is a function that calculates embeddings\n",
    "based on the node labels an properties: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(driver, model):\n",
    "    with driver.session() as session:\n",
    "\n",
    "        # Retrieve all nodes\n",
    "        result = session.run(\"MATCH (n) RETURN n\")\n",
    "\n",
    "        for record in result:\n",
    "            node = record[\"n\"]\n",
    "            # Combine node labels and properties into a single string\n",
    "            node_data = (\n",
    "                \" \".join(node.labels)\n",
    "                + \" \"\n",
    "                + \" \".join(f\"{k}: {v}\" for k, v in node.items())\n",
    "            )\n",
    "\n",
    "            # Compute the embedding for the node\n",
    "            node_embedding = model.encode(node_data)\n",
    "\n",
    "            # Store the embedding back into the node\n",
    "            session.run(\n",
    "                f\"MATCH (n) WHERE id(n) = {node.element_id} SET n.embedding = {node_embedding.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Set the label to Entity for all nodes\n",
    "        session.run(\"MATCH (n) SET n:Entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a node `:Character {name:\"Viserys Targaryen\"}` in the graph, the\n",
    "encoded embedding will include the label `:Charater` and the property\n",
    "`name:Viserys Targaryen`.\n",
    "\n",
    "Asking the question `Who is Viserys Targaryen?` will yield a very similar\n",
    "embedding, allowing you to locate that node in the graph. However, if you ask a\n",
    "longer question like, `To whom was Viserys Targaryen Loyal in seasone 1 of Game\n",
    "of Thrones?`, there is a chance that this question might not locate the `Viserys\n",
    "Targaryen` node in the graph due to its length and complexity. \n",
    "\n",
    "Embedding a triplet on the node will yield a better result in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the relevant part of the graph\n",
    "\n",
    "TODO: configure and set vector search index based on new release\n",
    "\n",
    "Once embeddings are calculated in your graph, you can perform a search based on\n",
    "these embeddings by using a vector search. \n",
    "\n",
    "Memgraph supports vector search starting from version 2.22.  \n",
    "\n",
    "The goal is to find the most similar node that resembles your question and to\n",
    "extract the relevant knowledge from it. The function takes the question's\n",
    "embedding and compares it to the embeddings stored on the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_node(driver, question_embedding):\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Perform the vector search on all nodes based on the question embedding\n",
    "        result = session.run(\n",
    "            f\"CALL vector_search.search('tag', 10, {question_embedding.tolist()}) YIELD * RETURN *;\"\n",
    "        )\n",
    "        nodes_data = []\n",
    "        \n",
    "        # Retrieve all similar nodes and print them\n",
    "        for record in result:\n",
    "            node = record[\"node\"]\n",
    "            properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "            node_data = {\n",
    "                \"distance\": record[\"distance\"],\n",
    "                \"id\": node.element_id,\n",
    "                \"labels\": list(node.labels),\n",
    "                \"properties\": properties,\n",
    "            }\n",
    "            nodes_data.append(node_data)\n",
    "        print(\"All similar nodes:\")\n",
    "        for node in nodes_data:\n",
    "            print(node)\n",
    "\n",
    "        # Return the most similar node\n",
    "        return nodes_data[0] if nodes_data else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the similarity between the question embeddings and node embeddings, we\n",
    "get the most similar node. This node serves as a pivot point from which we can\n",
    "pull relevant data. For example, if we are searching for information about\n",
    "`Viserys Targaryen`, we would pull data surrounding that node, making it our\n",
    "pivot node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the relevant data\n",
    "\n",
    "Once we have the pivot node, we can begin retrieving the relevant structured\n",
    "data around it. The most straightforward approach is to perform multiple hops\n",
    "from the pivot node. \n",
    "\n",
    "Here is the function that fetches the data around pivot node, a specified number\n",
    "of `hops` away from the pivot node.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_data(driver, node, hops):\n",
    "    with driver.session() as session:\n",
    "        # Retrieve the paths from the node to other nodes that are 'hops' away\n",
    "        query = (\n",
    "            f\"MATCH path=((n)-[r*..{hops}]-(m)) WHERE id(n) = {node['id']} RETURN path\"\n",
    "        )\n",
    "        result = session.run(query)\n",
    "\n",
    "        paths = []\n",
    "        for record in result:\n",
    "            path_data = []\n",
    "            for segment in record[\"path\"]:\n",
    "\n",
    "                # Process start node without 'embedding' property\n",
    "                start_node_data = {\n",
    "                    k: v for k, v in segment.start_node.items() if k != \"embedding\"\n",
    "                }\n",
    "\n",
    "                # Process relationship data\n",
    "                relationship_data = {\n",
    "                    \"type\": segment.type,\n",
    "                    \"properties\": segment.get(\"properties\", {}),\n",
    "                }\n",
    "\n",
    "                # Process end node without 'embedding' property\n",
    "                end_node_data = {\n",
    "                    k: v for k, v in segment.end_node.items() if k != \"embedding\"\n",
    "                }\n",
    "\n",
    "                # Add to path_data as a tuple (start_node, relationship, end_node)\n",
    "                path_data.append((start_node_data, relationship_data, end_node_data))\n",
    "\n",
    "            paths.append(path_data)\n",
    "\n",
    "        # Return all paths\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert a picture showing this. \n",
    "\n",
    "To avoid overloading the LLM's limited context with non-relevant data, we drop\n",
    "the embedding property from the nodes. Embeddings contain a lot of data that\n",
    "isn't particularly relevant to the LLM. \n",
    "\n",
    "## Helper functions \n",
    "\n",
    "For the LLM to understand its task, we need specific prompts. The `RAG_prompt`\n",
    "describes how the LLM should answer the question, while the `question_prompt` is\n",
    "optimized for calculating question embeddings by extracting only the key pices\n",
    "of information to improve embedding accuracy. For example, if you ask, `Who is\n",
    "Viserys Targaryen?`, only the `Viserys Targaryen` will be extracted from the\n",
    "question. Ultimately, the LLM will receive the full question back in the\n",
    "`RAG_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_prompt(question, relevance_expansion_data):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI language model. I will provide you with a question and a set of data obtained through a relevance expansion process in a graph database. The relevance expansion process finds nodes connected to a target node within a specified number of hops and includes the relationships between these nodes.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Relevance Expansion Data:\n",
    "    {relevance_expansion_data}\n",
    "\n",
    "    Based on the provided data, please answer the question, make sure to base your answers only based on the provided data. Add a context on what data did you base your answer on.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def question_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI language model. I will provide you with a question. \n",
    "    Extract the key information from the questions. The key information is important information that is required to answer the question.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    The output format should be like this: \n",
    "    Key Information: [key information 1], [key information 2], ...\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "async def get_response(client, prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the graph RAG\n",
    "\n",
    "Now, it all comes together in the `main` function: \n",
    "\n",
    "1. Connect to the database \n",
    "2. Load the .env file with the `OPENAI_API_KEY=` defined\n",
    "3. Compute and store the node embeddings \n",
    "4. Compute the question embedding based on key information \n",
    "5. Perform the vector search to find the most semantically similar node\n",
    "6. Get the relevant data that is a few hops away from the pivot node\n",
    "7. Ask LLM the question with the relevant data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting a Graph RAG answer \n",
    "\n",
    "def main():\n",
    "\n",
    "    # Create a Neo4j driver\n",
    "    driver = neo4j.GraphDatabase.driver(\"bolt://100.64.149.141:7687\", auth=(\"\", \"\"))\n",
    "\n",
    "    # Load .env file\n",
    "    load_dotenv()\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # Load the SentenceTransformer model\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "    # Compute embeddings for all nodes in the graph\n",
    "    compute_bigger_embeddings_based_on_node(\n",
    "        driver, model\n",
    "    )\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    # Ask a question  (feel free to change the question) \n",
    "    question = \"In which episode was Viserys Targaryen killed?\"\n",
    "\n",
    "    # Key information from the question \n",
    "    prompt = question_prompt(question)\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    print(response)\n",
    "    key_information = response.split(\"Key Information: \")[1].strip()\n",
    "\n",
    "    # Compute the embedding for the key information\n",
    "    question_embedding = model.encode(key_information)\n",
    "\n",
    "    # Find the most similar node to the question embedding\n",
    "    node = find_most_similar_node(driver, question_embedding)\n",
    "    if node:\n",
    "        print(\"The most similar node is:\")\n",
    "        print(node)\n",
    "\n",
    "    # Get the relevant data based on the most similar node\n",
    "    relevant_data = get_relevant_data(driver, node, hops=2)\n",
    "\n",
    "    # Show the relevant data\n",
    "    print(\"The relevant data is:\")\n",
    "    print(relevant_data)\n",
    "\n",
    "    # LLM answers the question based on the relevant data\n",
    "    prompt = RAG_prompt(question, relevant_data)\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    print(\"The response is:\")\n",
    "    print(response)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are a few examples of questions and answers: \n",
    "\n",
    "**To whom was Viserys Targaryen loyal to?**\n",
    "\n",
    "The response is:\n",
    "Based on the provided data, Viserys Targaryen was loyal to House Targaryen. This information is derived from the relationships indicating that Viserys Targaryen was loyal to House Targaryen and the connections between them\n",
    "\n",
    "**Who killed Viserys Targaryen in Game of thrones?**\n",
    "\n",
    "The response is:\n",
    "\n",
    "**Based on the provided relevance expansion data, Khal Drogo killed Viserys Targaryen in \"Game of Thrones.\" This information is inferred from the relationship where Khal Drogo is linked to Viserys Targaryen with the action of being \"KILLED\" by Khal Drogo. The data does not show any other character directly killing Viserys Targaryen.\n",
    "**\n",
    "\n",
    "**\"What was the weapon used to kill Viserys Targaryen in Game of Thrones?\"**\n",
    "\n",
    "The response is: \n",
    "\n",
    "Based on the provided data, the weapon used to kill Viserys Targaryen in Game of Thrones was not explicitly mentioned. The data only shows that Khal Drogo was involved in the killing of Viserys Targaryen. There is no specific mention of the weapon used in the relevance expansion data. Therefore, I do not have enough information to answer the question about the weapon used to kill Viserys Targaryen.\n",
    "\n",
    "This response is wrong, there is a method mentioned, not weapon, but LLM didn't catch the context due to different naming. \n",
    "\n",
    "**\"Who betrayed Viserys Targaryen in Game of Thrones?\"**\n",
    "\n",
    "Based on the provided data, Khal Drogo betrayed Viserys Targaryen in Game of Thrones by killing him. This conclusion is drawn from the relationship between Khal Drogo and Viserys Targaryen where it is stated that Khal Drogo killed Viserys Targaryen.\n",
    "\n",
    "This response is based on the killing relationship, but betrayal could have different consequences. \n",
    "\n",
    "Let's expand this knowledge. \n",
    "\n",
    "**\"Who betrayed Viserys Targaryen in Game of Thrones?\"**\n",
    "\n",
    "Based on the provided data, Khal Drogo betrayed Viserys Targaryen in Game of Thrones. This conclusion is derived from the relationship between Viserys Targaryen and Khal Drogo, where Khal Drogo is connected to Viserys Targaryen through the 'BETRAYED_BY' relationship, indicating that Khal Drogo betrayed Viserys Targaryen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding the knowledge\n",
    "\n",
    "Let's say that now we want to expand our existing knowledge graph with\n",
    "additional information to enrich the dataset, provide more context and retrieve\n",
    "more relevant data. In this example, we will take unstructured data, such as the\n",
    "character description summary provided below, extract entities from that\n",
    "summary, generate triplets to build the knowledge graph create queries and\n",
    "eventually execute those queries in Memgraph to incorporate with the existing\n",
    "graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text summary for processing\n",
    "summary = \"\"\"\n",
    "    Viserys Targaryen is the last living son of the former king, Aerys II Targaryen (the 'Mad King').\n",
    "    As one of the last known Targaryen heirs, Viserys Targaryen is obsessed with reclaiming the Iron Throne and \n",
    "    restoring his family’s rule over Westeros. Ambitious and arrogant, he often treats his younger sister, Daenerys Targaryen, \n",
    "    as a pawn, seeing her only as a means to gain power. His ruthless ambition leads him to make a marriage alliance with \n",
    "    Khal Drogo, a powerful Dothraki warlord, hoping Khal Drogo will give him the army he needs. \n",
    "    However, Viserys Targaryen’s impatience and disrespect toward the Dothraki culture lead to his downfall;\n",
    "    he is ultimately killed by Khal Drogo in a brutal display of 'a crown for a king' – having molten gold poured over his head. \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction\n",
    "\n",
    "TODO: add links \n",
    "\n",
    "The first step in the process is to extract entities from the summary using\n",
    "SpaCy’s LLM.\n",
    "\n",
    "To begin, we need to install SpaCy and the specific model we wll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install spacy_llm\n",
    "%python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of extracting entities from the text is to preprocess the data before\n",
    "sending it to the GPT model, ensuring more accurate and relevant results. By\n",
    "using SpaCy, we can identify key entities such as characters, locations and\n",
    "other entities for better understanding of the context of the text.\n",
    "\n",
    "This is particularly useful because SpaCy is specifically trained to recognize\n",
    "linguistic patterns and relationships in text, which helps to isolate and\n",
    "highlight the most important pieces of information. By preprocessing the text\n",
    "this way, we ensure that the GPT model receives a more structured input, helps\n",
    "reduce noise and irrelevant data, leading to more precise and context-aware\n",
    "outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy_llm.util import assemble\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Split document into sentences\n",
    "def split_document_sent(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def process_text(text, nlp, verbose=False):\n",
    "    doc = nlp(text)\n",
    "    if verbose:\n",
    "        print(f\"Text: {doc.text}\")\n",
    "        print(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Pipeline to run entity extraction\n",
    "def extract_entities(text, nlp, verbose=False):\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "\n",
    "    sentences = split_document_sent(text, nlp)\n",
    "    for sent in sentences:\n",
    "        doc = process_text(sent, nlp, verbose)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # Store processed data for each sentence\n",
    "        processed_data.append({\"text\": doc.text, \"entities\": entities})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open(\"processed_data.json\", \"w\") as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract node and relationship parameters\n",
    "\n",
    "Now that we have extracted entities from the text, we have a better\n",
    "understanding of the data and a more structured context to send to GPT model\n",
    "we'll be using. The next step is to provide the extracted JSON file to the GPT\n",
    "prompt, along with clear instructions on how to extract nodes and relationships\n",
    "from those entities. These instructions will guide the model in identifying key\n",
    "connections between the entities, which can then be used to build a knowledge\n",
    "graph. In this example, we will be using a gpt-4 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Load the spaCy model\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    extract_entities(summary, nlp)\n",
    "\n",
    "\n",
    "    # Load processed data from JSON\n",
    "    json_path = Path(\"processed_data.json\")\n",
    "    with open(json_path, \"r\") as f:\n",
    "        processed_data = json.load(f)\n",
    "\n",
    "    # Prepare nodes and relationships\n",
    "    nodes = []\n",
    "    relationships = []\n",
    "\n",
    "    # Formulate a prompt for GPT-4\n",
    "    prompt = (\n",
    "        \"Extract entities and relationships from the following JSON data. For each entry in data['entities'], \"\n",
    "        \"create a 'node' dictionary with fields 'id' (unique identifier), 'name' (entity text), and 'type' (entity label). \"\n",
    "        \"For entities that have meaningful connections, define 'relationships' as dictionaries with 'source' (source node id), \"\n",
    "        \"'target' (target node id), and 'relationship' (type of connection). Create max 30 nodes, format relationships in the format of capital letters and _ inbetween words and format the entire response in the JSON output containing only variables nodes and relationships without any text inbetween. Use following labels for nodes: Character, Title, Location, House, Death, Event, Allegiance and following relationship types: HAPPENED_IN, SIBLING_OF, PARENT_OF, MARRIED_TO, HEALED_BY, RULES, KILLED, LOYAL_TO, BETRAYED_BY. Make sure the entire JSON file fits in the output\"\n",
    "        \"JSON data:\\n\"\n",
    "        f\"{json.dumps(processed_data)}\"\n",
    "    )\n",
    "\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    structured_data = json.loads(response)  # Assuming GPT-4 outputs structured JSON\n",
    "\n",
    "    # Populate nodes and relationships lists\n",
    "    nodes.extend(structured_data.get(\"nodes\", []))\n",
    "    relationships.extend(structured_data.get(\"relationships\", []))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate queries\n",
    "\n",
    "Now that GPT has provided us with the structured data for the nodes and\n",
    "relationships, the next step is to generate the Cypher queries that we will use\n",
    "to execute in Memgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher_queries(nodes, relationships):\n",
    "    queries = []\n",
    "\n",
    "    # Create nodes\n",
    "    for node in nodes:\n",
    "        query = f\"\"\"\n",
    "        MERGE (n:{node['type']}:Entity {{name: '{node['name']}'}}) \n",
    "        ON CREATE SET n.id={node['id']} \n",
    "        ON MATCH SET n.id={node['id']}\n",
    "        \"\"\"\n",
    "        queries.append(query)\n",
    "\n",
    "    # Create relationships\n",
    "    for rel in relationships:\n",
    "        query = f\"MATCH (a {{id: {rel['source']}}}), (b {{id: {rel['target']}}}) \" \\\n",
    "                f\"CREATE (a)-[:{rel['relationship']}]->(b)\"\n",
    "        queries.append(query)\n",
    "\n",
    "    return queries\n",
    "\n",
    "cypher_queries = generate_cypher_queries(nodes, relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute queries\n",
    "\n",
    "The final step is to execute those queries in Memgraph, enriching your graph\n",
    "with the newly created context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    for query in cypher_queries:\n",
    "        try:\n",
    "            session.run(query)\n",
    "            print(f\"Executed query: {query}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {query}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now has additional knowledge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MATCH (a {id: 1}), (b {id: 2}) CREATE (a)-[:PARENT_OF]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 3}) CREATE (a)-[:SIBLING_OF]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 4}) CREATE (a)-[:LOYAL_TO]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 5}) CREATE (a)-[:HAPPENED_IN]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 6}) CREATE (a)-[:HAPPENED_IN]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 7}) CREATE (a)-[:SIBLING_OF]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 8}) CREATE (a)-[:MARRIED_TO]->(b)\n",
    "MATCH (a {id: 1}), (b {id: 9}) CREATE (a)-[:HEALED_BY]->(b)\n",
    "MATCH (a {id: 8}), (b {id: 9}) CREATE (a)-[:RULES]->(b)\n",
    "MATCH (a {id: 8}), (b {id: 1}) CREATE (a)-[:BETRAYED_BY]->(b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now again asking the same question yields a correct answer: **\"Who betrayed Viserys Targaryen in Game of Thrones?\"**\n",
    "\n",
    "Based on the provided data, Khal Drogo betrayed Viserys Targaryen in Game of Thrones. This conclusion is derived from the relationship between Viserys Targaryen and Khal Drogo, where Khal Drogo is connected to Viserys Targaryen through the 'BETRAYED_BY' relationship, indicating that Khal Drogo betrayed Viserys Targaryen.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('kg_examp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bdc9527c232110913f250aa506ce0def2a2883fef9960f0ee91861d9de544f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
